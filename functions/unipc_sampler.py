from typing import Optional,Tuple
import torch, tqdm
from diffusers.schedulers.scheduling_unipc_multistep import UniPCMultistepScheduler
from functions.sde import VPSDE1D


def make_unipc_scheduler(
        sde: VPSDE1D,                    
        num_train_steps: int = 1_000,
        device: str | torch.device = "cuda",
        *,                           
        solver_order: int = 4,       #   2Â·3Â·4 ì§€ì›
        lower_order_final: bool = True,
) -> UniPCMultistepScheduler:
    """
    HDM VPSDE1Dì™€ ìˆ˜ì¹˜ì ìœ¼ë¡œ ë™ì¼í•œ Î±Ì„(t), Î²(t) ì´ì‚°í™” í…Œì´ë¸”ì„ ê°–ëŠ”
    UniPC-Multistep Schedulerë¥¼ ìƒì„±í•œë‹¤.
    ----------
    - t_i = (i+1)/N Â· T  (i=0,â€¦,Nâˆ’1)
    - Î±Ì„â‚€ = 1,  Î²_i = 1 âˆ’ Î±Ì„(t_i)/Î±Ì„(t_{iâˆ’1})
    """
    T = sde.T                              # 0.9946  (cosine schedule)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 0 â‰¤ tâ‚€ < â€¦ < t_N = T  (ê¸¸ì´ N+1)
    # Î²_i := 1 âˆ’ Î±Ì„(t_i)/Î±Ì„(t_{iâˆ’1})   (i = 1â€¦N)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ts = torch.linspace(
        0.0, T, num_train_steps + 1,         # shape (N+1,)
        dtype=torch.float64, device=device
    )

    log_alpha = sde.marginal_log_mean_coeff(ts)          # ln Î±Ì„(t)
    alpha_bar = torch.exp(log_alpha)                     # Î±Ì„(t)

    alpha_bar_t = alpha_bar[1:]
    alpha_bar_t_minus_1 = alpha_bar[:-1]
    
    # beta_t = 1 - (alpha_bar_t / alpha_bar_t-1)
    # ë¶„ëª¨ê°€ 0ì´ ë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê³ , ë¹„ìœ¨ì´ 1ì„ ë„˜ì§€ ì•Šë„ë¡ clamp
    beta_t = 1.0 - torch.clamp(alpha_bar_t / alpha_bar_t_minus_1, max=0.9999)
    
    # ìµœì¢… betas ê°’ë„ ì•ˆì •ì ì¸ ë²”ìœ„ ë‚´ë¡œ í´ë¨í•‘
    betas = torch.clamp(beta_t, 0.0, 0.9999)

    # diffusers ëŠ” CPU tensor/numpy ë¥¼ ê¸°ëŒ€ â†’ .cpu()
    scheduler = UniPCMultistepScheduler(
        trained_betas       = betas.float().cpu(),
        num_train_timesteps = len(betas),   # = N
        solver_type         = "bh2",        # ODE ëª¨ë“œ
        solver_order        = solver_order, # â˜… 3 ë˜ëŠ” 4 ì°¨
        lower_order_final   = lower_order_final,
        prediction_type     = "epsilon",
        thresholding        = False,
        sample_max_value    = 1.0,
    )  
    return scheduler


@torch.no_grad()
def sample_probability_flow_ode(
        model,
        sde: VPSDE1D,
        x_t0: torch.Tensor, 
        x_coord: torch.Tensor,       
        batch_size: Optional[int] = None,
        data_dim: Optional[int] = None,
        device: torch.device = torch.device("cuda"),
        inference_steps: int = 500,
        fp16: bool = False,
        progress: bool = True, 
):
    """
    Args
    ----
    model            : score network,  (B, N) â†’ (B, N)
    sde              : VPSDE1D (ì‹œê°„ ìŠ¤ì¼€ì¼ T=0.9946 ê¸°ì¤€)
    inference_steps  : NFE (= scheduler steps)
    x_t0             : None ì´ë©´ N(0, I) ì—ì„œ ìƒ˜í”Œë§
    Returns
    -------
    x_0 (torch.Tensor) : (B, data_dim) ë³µì› ìƒ˜í”Œ
    """

    scheduler = make_unipc_scheduler(
        sde,
        num_train_steps=1000,
        device=device,
        solver_order=4,              # í•„ìš” ì‹œ 4ë¡œ ì¡°ì •
        lower_order_final=True,
    )
    scheduler.set_timesteps(inference_steps, device=device)

    # ì´ˆê¸°ë¶„í¬: p_T = ğ’©(0, Ïƒ_TÂ² I)
    if x_t0 is None:
        if batch_size is None or data_dim is None:
            raise ValueError("batch_size and data_dim must be provided if x_t0 is not given.")
        sigma_target = 1.0
        
        is_model_2d = hasattr(model, 'n_dim') and model.n_dim == 2
        if is_model_2d:
            x = torch.randn(batch_size, data_dim, data_dim, device=device) * sigma_target
        else:
            x = torch.randn(batch_size, data_dim, device=device) * sigma_target
    else:
        x = x_t0.to(device)
        x_coord = x_coord.to(device)
        # x_t0ì—ì„œ shape ì •ë³´ë¥¼ ì¶”ë¡ 
        batch_size = x.shape[0]

    is_2d = (x.dim() == 3)

    model.eval()
    autocast = torch.cuda.amp.autocast if fp16 else torch.no_grad

    for i, t in enumerate(tqdm.tqdm(scheduler.timesteps, disable=not progress)):
        with autocast():
            #   scheduler.timesteps â†’ {N-1, â€¦, 0}
            #   ì‹¤ì œ ì‹œê°   táµ¢ = (i+1)/N Â· T  (cosine VPSDE)
            t_cont = t.to(torch.float32) / scheduler.config.num_train_timesteps * sde.T
            t_vec  = t_cont.repeat(batch_size).to(device).to(x.dtype)       # shape (B,)
            # [ìˆ˜ì •] 2D/1Dì— ë”°ë¼ ëª¨ë¸ ì…ë ¥ í˜•íƒœ ë³€ê²½
            if is_2d:
                # 2D Case: x(B,H,W), x_coord(B,H,W,2) -> model_input(B,3,H,W)
                x_coord_ch = x_coord.permute(0, 3, 1, 2) # (B, 2, H, W)
                model_input = torch.cat([x.unsqueeze(1), x_coord_ch], dim=1)
                score = model(model_input, t_vec).squeeze(1) # model output: (B,1,H,W) -> (B,H,W)
            else:
                # 1D Case: x(B,N), x_coord(B,N,1) -> model_input(B,2,N)
                model_input = torch.cat([x.unsqueeze(1), x_coord.unsqueeze(1)], dim=1)
                score = model(model_input, t_vec)

        # diffusers expects epsilon â†’ Îµ = - score
        sigma_t = scheduler.sigmas.to(device)[i]   
        epsilon = - score
        x = scheduler.step(epsilon, t, x).prev_sample

        flat  = x.view(x.size(0), -1)            # (B, N) ë¡œ í´ê¸°
        norm  = flat.norm(dim=1, keepdim=True) 
        if is_2d:
            H, W = x.shape[1], x.shape[2]
            threshold = 10.0 * math.sqrt(H * W)
        else:
            threshold = 10.0          # ê° ìƒ˜í”Œ L2-norm
        mask = (norm > threshold).squeeze(1)          # Quadratic ì€ ì„ê³„ 10 
        flat[mask] = flat[mask] / norm[mask] * threshold
        x = flat.view_as(x)

    return x

